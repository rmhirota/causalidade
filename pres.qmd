---
title: "Double Machine Learning"
format:
  revealjs: 
    theme: solarized
---

## Motivação

. . .

Suponhamos que temos um modelo linear:

$$
Y_i = \tau T_i + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + e_i
$$

em que $\tau$ é o efeito causal que temos interesse em estimar.

. . .

<br><br>

Pode ser que o número de covariáveis a serem ajustadas seja muito grande e o trabalho de feature engineering seja complexo (decidir quais interações colocar, se os termos polinomiais de grau maior que 2 serão inclusos ou não, etc.)

## Teorema: Frisch-Waugh-Lovell

. . . 

### Intuição

A partir do modelo inicial, 

$$
Y_i = \tau T_i + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + e_i
$$

vamos ajustar dois modelos de regressão linear:

1. $Y_i \sim X_1 + X_2 + X_3$ 
1. $T_i \sim X_1 + X_2 + X_3$ 

Temos resíduo do modelo (1), $r_Y = Y - \hat{Y}$ e o resíduo do modelo (2), $r_T = T - \hat{T}$


A regressão do $r_Y$ em $r_T$ resulta em $\tau$

## Exemplo

```{r}
#| label: ex1
#| echo: true

sample_n <- 10000

x1 <- rnorm(n = sample_n)
x2 <- rnorm(n = sample_n, 10)
x3 <- rnorm(n = sample_n, 4)
t <- rnorm(sample_n, 5, .5)
y <- t * 2 + 0.2 * x1 + 0.5 * x2 + 10 * x3 + rnorm(sample_n)

fit_y <- lm(y ~ x1 + x2 + x3)
fit_t <- lm(t ~ x1 + x2 + x3)

fit <- lm(resid(fit_y) ~ resid(fit_t))
summary(fit)
```

## Restrições 

- Linearidade
- Número de parâmetros/variáveis

. . .

### Exemplo

(colocar exemplo do Facure -- temperatura)  

- Feature engineering  
- Interação

## Alternativa

Modelos não paramétricos para obter os resíduos $r_Y$ e $r_T$

- Árvores
- Bagging
- etc

## Por que podemos generalizar para modelos não paramétricos?

Prova / exemplo

## Restrições clássicas de modelos paramétricos

- Overfitting

. . . 

### Possíveis soluções

- Cross validation


## Simulação de cenários reais


